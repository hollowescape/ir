# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MivPmX8mrec00S4zhmNi-I9hlOGxQx3c
"""

with open('/content/Text1.txt',encoding="utf8",errors="ignore") as f:
  text1 = f.read()
with open('/content/Text2.txt',encoding="utf8",errors="ignore") as f1:
  text2 = f1.read()
with open('/content/Text3.txt',encoding="utf8",errors="ignore") as f2:
  text3 = f2.read()

import re
import string

text1_lower = text1.lower()
text2_lower = text2.lower()
text3_lower = text3.lower()

def count_words(string):
  return (len(re.findall('[a-z]+',string)))

def count_digits(string):
  return(len(re.findall('[0-9]+',string)))

text1_lower_wordcount = count_words(text1_lower)
text2_lower_wordcount = count_words(text2_lower)
text3_lower_wordcount = count_words(text3_lower)

print(f'The number of words in file1 is {text1_lower_wordcount} and count of digits in file1 are {count_digits(text1_lower)}')
print(f'The number of words in file3 is {text2_lower_wordcount} and count of digits in file2 are {count_digits(text2_lower)}')
print(f'The number of words in file3 is {text3_lower_wordcount} and count of digits in file3 are {count_digits(text3_lower)}')

"""Removing the numbers, punctation , whitespace"""

def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

def remove_whitespace(text):
    return  " ".join(text.split())

text1_rn = remove_numbers(text1_lower)
text1_rp = remove_punctuation(text1_rn)
text1_rw = remove_whitespace(text1_rp)

text2_rn = remove_numbers(text2_lower)
text2_rp = remove_punctuation(text2_rn)
text2_rw = remove_whitespace(text2_rp)

text3_rn = remove_numbers(text3_lower)
text3_rp = remove_punctuation(text3_rn)
text3_rw = remove_whitespace(text3_rp)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    return filtered_text

text1_rmstopword = remove_stopwords(text1_rw)
text2_rmstopword = remove_stopwords(text2_rw)
text3_rmstopword =  remove_stopwords(text3_rw)

filtered_text1_wordcount = len(text1_rmstopword)
filtered_text2_wordcount = len(text2_rmstopword)
filtered_text3_wordcount = len(text3_rmstopword)

print(f'After tokenization file1 wordcount : {filtered_text1_wordcount}')
print(f'After tokenization file2 wordcount: {filtered_text2_wordcount}')
print(f'After tokenization file3 wordcount: {filtered_text3_wordcount}')

def percentage_decrease(filtered_wordcount,unfiltered_wordcount):
  return ((1-(filtered_wordcount/unfiltered_wordcount))*100)

text1_lower_wordcount_decrease_percentage = percentage_decrease(filtered_text1_wordcount,text1_lower_wordcount)
text2_lower_wordcount_decrease_percentage = percentage_decrease(filtered_text2_wordcount,text2_lower_wordcount)
text3_lower_wordcount_decrease_percentage = percentage_decrease(filtered_text3_wordcount,text3_lower_wordcount)

print(f'The percentage decrease in file1 after stopword elimination is: {text1_lower_wordcount_decrease_percentage}%')
print(f'The percentage decrease in file2 after stopword elimination is: {text2_lower_wordcount_decrease_percentage}%')
print(f'The percentage decrease in file3 after stopword elimination is: {text3_lower_wordcount_decrease_percentage}%')

from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer = PorterStemmer()

def stem_words(text):
    word_tokens = word_tokenize(text)
    stems = [stemmer.stem(word) for word in word_tokens]
    return stems

text1_rmstemword = stem_words(text1_rw)
text2_rmstemword = stem_words(text2_rw)
text3_rmstemword =  stem_words(text3_rw)

"""Similarly we can do for this """

print(text1_rmstopword)

print(text1_rmstemword)

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')

def lemmatize_word(text):
    word_tokens = word_tokenize(text)
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]
    return lemmas

text1_lem = lemmatize_word(text1_rw)
text2_lem = lemmatize_word(text2_rw)
text3_lem = lemmatize_word(text3_rw)

"""inverterd index"""

def create_inverted_index(filtered_stemmed_list):

  inverted = {}
  for index,word in enumerate(filtered_stemmed_list):
    locations = inverted.setdefault(word, [])
    locations.append(index)
  return inverted

print(f"For file1 stemmed text \n {create_inverted_index(text1_rmstemword)}")
print(f"For file2 stemmed text \n {create_inverted_index(text2_rmstemword)}")
print(f"For file3 stemmed text \n {create_inverted_index(text3_rmstemword)}")



"""Vector model"""

import pandas as pd
import sklearn as sk
import math

def create_corpus(d1,d2):
  return set(d1).union(set(d2))

d1 = ["new","york","times"]
d2 = ["new", "york", "post"]
corpus = create_corpus(d1,d2)

def word_in_corpus(corpus,d1,d2):
  wordDictA = dict.fromkeys(corpus, 0) 
  wordDictB = dict.fromkeys(corpus, 0)
  for word in d1:
    wordDictA[word] = wordDictA[word] + 1
  for word in d2:
    wordDictB[word] = wordDictB[word] + 1
  return [wordDictA,wordDictB]

word_in_corpus_list = word_in_corpus(corpus,d1,d2)
pd.DataFrame(word_in_corpus_list)

def get_TF(wordDict,doc):
  tfDict = {}
  doc_length = len(doc)
  for word,count in wordDict.items():
    tfDict[word] = count/float(doc_length)
  return tfDict

tfFirst = get_TF(word_in_corpus_list[0],d1)
tfSecond = get_TF(word_in_corpus_list[1],d2)
pd.DataFrame([tfFirst,tfSecond])

def get_idf(wordDict):
  idfDict = {}
  N = len(wordDict)
  idfDict = dict.fromkeys(wordDict[0].keys(), 0)
  for word,val in idfDict.items():
    idfDict[word] = math.log(N / (float(val)+1))
  return idfDict

idfs = get_idf([word_in_corpus_list[0],word_in_corpus_list[1]])
print(idfs)

def get_df_idf(term_frequency,inverse_document_frequency):
  tfIdf = {}
  for word,value in term_frequency.items():
    tfIdf[word] = value * inverse_document_frequency[word]
  return tfIdf

idfFirst = get_df_idf(tfFirst,idfs)
idfSecond = get_df_idf(tfSecond,idfs)
pd.DataFrame([idfFirst,idfSecond])

def get_length(document_idf_list):
  length_list = []
  for idf_document in document_idf_list:
    length = 0
    for idf in idf_document.values():
      length = (idf*idf) + length
    length = math.sqrt(length) 
    length_list.append(length)
  return length_list

print(get_length([idfFirst,idfSecond]))

import numpy as np

def mul(list):
  answer = 1
  for i in list:
    answer = i*answer
  return answer

def getCosSim(idf1,idf2):
  n1 = np.array(list(idf1.values()))
  n2 = np.array(list(idf2.values()))
  return sum(n1*n2)/mul(get_length([idf1,idf2]))

print(getCosSim(idfFirst,idfSecond))

"""Probability model"""

d1 = ["a","b","c","b","d"]
d2 = ["b","e","f","b"]
d3 = ["b","g","c","d"]
d4 = ["b","d","e"]
d5 = ["a","b","e","g"]
d6 = ["b","g","h"]
q = ["a","c","h"]

def create_corpus_from_list(list_of_documents):
  answer = [set(li) for li in list_of_documents]
  res = set().union(*answer)
  return res

def get_N(list_of_documents):
  return len(list_of_documents)

def sort_set(set):
  return sorted(list(set))

print(sort_set(create_corpus_from_list([d1,d2,d3,d4,d5,d6])))

def get_nw(corpus_set,list_of_documents):
  answer_dict = {}
  for word in corpus_set:
    num = 0
    for li in list_of_documents:
      if word in li:
        num += 1
    answer_dict[word] = num
  return answer_dict

nw = (get_nw(sort_set(create_corpus_from_list([d1,d2,d3,d4,d5,d6])),[d1,d2,d3,d4,d5,d6]))
print(nw)

def find_relevance(N,nw):
  return (N-nw+0.5)/(nw+0.5)

def find_relevance_all_documents(N,nw_dictionary):
  relevance_dict = {}
  for key,values in nw_dictionary.items():
    relevance_dict[key] = find_relevance(N,values)
  return relevance_dict

relevance_nw_dict = find_relevance_all_documents(get_N([d1,d2,d3,d4,d5,d6]),nw_dictionary=nw)
print(relevance_nw_dict)
print(relevance_nw_dict['c'])

pd.DataFrame([nw,relevance_nw_dict])

def cum_mul(list):
  answer = 1
  for num in list:
    answer = answer*num
  return answer

def get_probability(query,document,relevance_dictionary):
  answer = {}
  common_words_values = []
  for word in query:
    if word not in document:
      pass
    else:
      common_words_values.append(relevance_dictionary[word])
  return cum_mul(common_words_values)

print(get_probability(q,d1,relevance_nw_dict))

def get_ranking(query,list_of_documents,relevance_dictionary):
  answer = {}
  for i in range(len(list_of_documents)):
    answer['document'+str(i+1)] = get_probability(query,list_of_documents[i],relevance_dictionary=relevance_dictionary)
  sort_answer = sorted(answer.items(), key=lambda x: x[1], reverse=True)
  return sort_answer

print(get_ranking(q,[d1,d2,d3,d4,d5,d6],relevance_nw_dict))